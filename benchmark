#!/usr/bin/env python3
import argparse
import csv
import subprocess
import sys
import time
import concurrent.futures
import threading
import multiprocessing
from func_timeout import func_timeout, FunctionTimedOut
from pathlib import Path
from typing import Optional

def run(cmd, cwd=None, capture=False, timeout=None):
    try:
        if capture:
            res = subprocess.run(
                cmd,
                cwd=cwd,
                check=True,
                text=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=timeout,
            )
            return res.stdout.strip()
        else:
            subprocess.run(
                cmd,
                cwd=cwd,
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=timeout,
            )
            return ""
    except subprocess.CalledProcessError as e:
        if capture:
            sys.stderr.write(f"Command failed: {' '.join(cmd)}\n{e.stderr}\n")
        else:
            sys.stderr.write(f"Command failed: {' '.join(cmd)}\n")
        return None
    except subprocess.TimeoutExpired:
        sys.stderr.write(f"Command timed out: {' '.join(cmd)}\n")
        return None


def count_points_n_pairs(path: Path) -> Optional[int]:
    try:
        with path.open('r') as f:
            header = f.readline()
            if not header:
                return None
            try:
                n = int(header.strip())
            except Exception:
                return None
            return n
    except Exception:
        return None


def needs_header(path: Path, expected_header: list[str]) -> bool:
    try:
        if not path.exists():
            return True
        with path.open('r', newline='') as f:
            first = f.readline()
            if not first:
                return True
            return first.strip() != ','.join(expected_header)
    except Exception:
        return True


def _read_curve(path: Path):
    """Read N + N lines of x y into numpy array; return None on failure or if lib unavailable."""
    try:
        # Import locally to avoid static analysis complaints when globals are None
        import numpy as _np  # type: ignore
        with path.open('r') as f:
            header = f.readline()
            if not header:
                return None
            n = int(header.strip())
            pts = _np.empty((n, 2), dtype=float)
            for i in range(n):
                line = f.readline()
                if not line:
                    return None
                parts = line.strip().split()
                if len(parts) < 2:
                    return None
                pts[i, 0] = float(parts[0])
                pts[i, 1] = float(parts[1])
            return pts
    except Exception:
        return None

def _calc_frechet_safe_task(orig, other, timeout_val):
    try:
        # Import locally; subsequent calls in the same process reuse sys.modules
        from frechetlib.continuous_frechet import frechet_c_approx as _frechet_c_approx
        
        def do_calc():
            res, morphing = _frechet_c_approx(orig, other, 1.01)
            dist = getattr(morphing, 'dist', res if isinstance(res, (int, float)) else None)
            return float(dist) if dist is not None else None

        # Use func_timeout to enforce timeout within the worker process via signals
        return func_timeout(timeout_val, do_calc)
    except FunctionTimedOut:
        return None
    except Exception:
        return None

def _frechet_distance(orig_path: Path, other_path: Path, pool, timeout=60.0):
    """Compute Frechet distance utilizing persistent process pool."""
    t0 = time.monotonic()
    orig = _read_curve(orig_path)
    other = _read_curve(other_path)
    if orig is None or other is None:
        return (None, time.monotonic() - t0)
    
    try:
        # Submit to pool
        async_res = pool.apply_async(_calc_frechet_safe_task, (orig, other, timeout))
        # Wait for result with a small buffer. If task complies with timeout, it returns.
        # If it hangs (signal blocked), this get() will timeout.
        val = async_res.get(timeout=timeout + 5.0)
        return (val, time.monotonic() - t0)
    except Exception:
        # Timeout or other error
        return (None, time.monotonic() - t0)


def process_id(idv, args, repo_root, binaries, csv_ctx, frechet_pool):
    main_bin, simplify_bin, dots_bin = binaries
    writers, files, locks = csv_ctx
    base_writer, fr_writer, base_t_writer, fr_t_writer = writers
    base_file, fr_file, base_t_file, fr_t_file = files
    points_lock, frechet_lock, base_times_lock, frechet_times_lock, print_lock = locks

    def log(msg, file=None):
        t_name = threading.current_thread().name
        if t_name.startswith('ThreadPoolExecutor-'):
            parts = t_name.split('_')
            if len(parts) > 1:
                t_name = f"W-{parts[-1]}"
        prefix = f"[{t_name} | ID:{idv}]"
        with print_lock:
            print(f"{prefix:<18} {msg}", file=file or sys.stdout)

    log("Starting...")
    # 1. Run algorithms to generate simplified file
    simp_dir = repo_root / 'data' / 'taxi_simplified' / str(idv)
    
    if args.algo == 'dots':
        log(f"1. Running DOTS for id={idv} threshold=500...")
        rc = run([str(dots_bin), str(idv), "500"])
        baseline_filename = 'dots_simplified.txt'
    else:
        log(f"1. Running main (DP) for id={idv} err=200...")
        rc = run([str(main_bin), str(idv), "200"])
        baseline_filename = 'dp_simplified.txt'

    log(f"    done")
    if rc is None:
        log(f"Skipping id={idv} due to algo failure", file=sys.stderr)
        for eps in args.eps:
             with points_lock:
                 base_writer.writerow([idv, -1, eps, -1, -1, -1, -1, -1])
                 base_file.flush()
             with frechet_lock:
                 fr_writer.writerow([idv, -1, eps, -1, -1, -1, -1, -1])
                 fr_file.flush()
             with base_times_lock:
                 base_t_writer.writerow([idv, eps, -1, -1, -1])
                 base_t_file.flush()
        return

    # Paths
    baseline_path = simp_dir / baseline_filename
    simp_path = simp_dir / 'simplified.txt'

    # 2. Baseline Frechet distance (in-process if possible)
    log(f"2. Calculating frechet distance for id={idv}...")
    original_path = simp_dir / 'original.txt'
    if not original_path.exists():
        log(f"    Missing original.txt for id={idv}", file=sys.stderr)
        for eps in args.eps:
             with points_lock:
                 base_writer.writerow([idv, -1, eps, -1, -1, -1, -1, -1])
                 base_file.flush()
             with frechet_lock:
                 fr_writer.writerow([idv, -1, eps, -1, -1, -1, -1, -1, 'missing_file'])
                 fr_file.flush()
             with base_times_lock:
                 base_t_writer.writerow([idv, eps, -1, -1, -1])
                 base_t_file.flush()
        return
    baseline_dist, baseline_time = _frechet_distance(original_path, baseline_path, frechet_pool, timeout=args.frechet_timeout)
    baseline_points = count_points_n_pairs(baseline_path)
    orig_points = count_points_n_pairs(original_path)
    
    if baseline_dist is None:
        log(f"    No baseline distance for id={idv} (or timed out)", file=sys.stderr)
        # Try to at least get points if possible
        bp_val = baseline_points if baseline_points is not None else -1
        op_val = orig_points if orig_points is not None else -1
        for eps in args.eps:
             with points_lock:
                 base_writer.writerow([idv, -1, eps, -1, -1, op_val, bp_val, -1])
                 base_file.flush()
             with frechet_lock:
                 fr_writer.writerow([idv, -1, eps, -1, -1, op_val, bp_val, -1, 'baseline_frechet_failed'])
                 fr_file.flush()
             with base_times_lock:
                 base_t_writer.writerow([idv, eps, -1, -1, -1])
                 base_t_file.flush()
        return
    log(f"    Frechet distance for baseline is {baseline_dist} (time={baseline_time:.6f}s)...")
    t_dp0 = 0.0  # maintain variables for CSV compatibility
    t_dp1 = baseline_time

    for eps in args.eps:
        log(f"-- ID {idv}, eps = {eps} --")
        # 3. Run simplify for each epsilon with delta = baseline_dist/(1+eps)
        delta = baseline_dist / (1.0 + eps)
        log(f"3. Running simplify with eps={eps} delta={delta}...")
        t_s0 = time.monotonic()
        rc2 = run([str(simplify_bin), '--in', str(idv), '--out', '-d', f"{delta}", '-e', f"{eps}"])
        t_s1 = time.monotonic()
        if rc2 is None:
            log(f"    simplify failed for id={idv}, eps={eps}", file=sys.stderr)
            op = orig_points if orig_points is not None else -1
            bp = baseline_points if baseline_points is not None else -1
            with points_lock:
                base_writer.writerow([idv, delta, eps, baseline_dist, -1.0, op, bp, -1])
                base_file.flush()
            with frechet_lock:
                fr_writer.writerow([idv, delta, eps, baseline_dist, -1.0, op, bp, -1, 'simplify_failed'])
                fr_file.flush()
            with base_times_lock:
                 base_t_writer.writerow([idv, eps, f"{t_dp1 - t_dp0:.6f}", f"{t_s1 - t_s0:.6f}", -1])
                 base_t_file.flush()
            continue

        log(f"    Calculating frechet distance for id={idv}...")
        simp_dist, simp_f_time = _frechet_distance(original_path, simp_path, frechet_pool, timeout=args.frechet_timeout)
        simp_points = count_points_n_pairs(simp_path)
        if simp_dist is None:
            log(f"    No simplified distance for id={idv}, eps={eps} (or timed out)", file=sys.stderr)
            with points_lock:
                # Use actual points counts if available, otherwise -1
                sp = simp_points if simp_points is not None else -1
                op = orig_points if orig_points is not None else -1
                bp = baseline_points if baseline_points is not None else -1
                base_writer.writerow([idv, delta, eps, baseline_dist, -1.0, op, bp, sp])
                base_file.flush()
            with frechet_lock:
                sp = simp_points if simp_points is not None else -1
                op = orig_points if orig_points is not None else -1
                bp = baseline_points if baseline_points is not None else -1
                fr_writer.writerow([idv, delta, eps, baseline_dist, -1.0, op, bp, sp, 'frechet_failed_or_timeout'])
                fr_file.flush()
            with base_times_lock:
                 base_t_writer.writerow([idv, eps, f"{t_dp1 - t_dp0:.6f}", f"{t_s1 - t_s0:.6f}", f"{simp_f_time:.6f}"])
                 base_t_file.flush()
            continue
        log(f"    Frechet distance for simplified is {simp_dist} (simplify_time={t_s1 - t_s0:.6f}s, frechet_time={simp_f_time:.6f}s)...")
        t_f0 = 0.0
        t_f1 = simp_f_time
        # simp_points already calculated above
        # 4. Write baseline CSV row (points baseline)
        log(f"4. Writing to minimize_points id={idv}...")
        with points_lock:
            base_writer.writerow([idv, delta, eps, baseline_dist, simp_dist, orig_points, baseline_points, simp_points])
            base_file.flush()
            
        with base_times_lock:
            # Write timing row for baseline
            base_t_writer.writerow([idv, eps, f"{t_dp1 - t_dp0:.6f}", f"{t_s1 - t_s0:.6f}", f"{t_f1 - t_f0:.6f}"])
            base_t_file.flush()
            
        if baseline_points is not None and simp_points is not None and simp_points > baseline_points:
             log(f"    Initial simp_points ({simp_points}) > baseline_points ({baseline_points}), skipping id={idv}...", file=sys.stderr)
             with frechet_lock:
                 fr_writer.writerow([idv, delta, eps, baseline_dist, simp_dist, orig_points, baseline_points, simp_points, 'exceed_baseline_points'])
                 fr_file.flush()
             continue

        # 5. Minimize Frechet distance while keeping points <= baseline_points by shrinking delta
        best_dist = simp_dist
        best_points = simp_points
        best_delta = delta
        cur_delta = delta * args.reduction
        iters = 0
        while iters < args.max_iters and best_delta > args.derr:
            log(f"5. Running simplify with eps={eps} delta={cur_delta}...")
            iters += 1
            t_s2 = time.monotonic()
            rc3 = run([str(simplify_bin), '--in', str(idv), '--out', '-d', f"{cur_delta}", '-e', f"{eps}"])
            t_s3 = time.monotonic()
            if rc3 is None:
                with frechet_times_lock:
                    fr_t_writer.writerow([idv, eps, iters, cur_delta, f"{t_s3 - t_s2:.6f}", '', '', '', 'simplify_failed'])
                    fr_t_file.flush()
                break
            log(f"    Calculating frechet distance for id={idv}...")
            simp_dist2, simp_iter_f_time = _frechet_distance(original_path, simp_path, frechet_pool, timeout=args.frechet_timeout)
            if simp_dist2 is None:
                with frechet_times_lock:
                    fr_t_writer.writerow([idv, eps, iters, cur_delta, f"{t_s3 - t_s2:.6f}", f"{simp_iter_f_time:.6f}", '', '', 'frechet_failed_or_timeout'])
                    fr_t_file.flush()
                break
            log(f"    Frechet distance for simplified is {simp_dist2} (simplify_time={t_s3 - t_s2:.6f}s, frechet_time={simp_iter_f_time:.6f}s)...")
            t_f2 = 0.0
            t_f3 = simp_iter_f_time
            simp_points2 = count_points_n_pairs(simp_path)
            if simp_points2 is None:
                with frechet_times_lock:
                    fr_t_writer.writerow([idv, eps, iters, cur_delta, f"{t_s3 - t_s2:.6f}", f"{t_f3 - t_f2:.6f}", simp_dist2, '', 'no_points'])
                    fr_t_file.flush()
                break
            # Stop if we exceed baseline_points
            if baseline_points is not None and simp_points2 > baseline_points:
                with frechet_times_lock:
                    fr_t_writer.writerow([idv, eps, iters, cur_delta, f"{t_s3 - t_s2:.6f}", f"{t_f3 - t_f2:.6f}", simp_dist2, simp_points2, ''])
                    fr_t_file.flush()
                break
            # Update best if distance improves
            if simp_dist2 < best_dist:
                best_dist = simp_dist2
                best_points = simp_points2
                best_delta = cur_delta
            # Record timing for this iteration
            with frechet_times_lock:
                fr_t_writer.writerow([idv, eps, iters, cur_delta, f"{t_s3 - t_s2:.6f}", f"{simp_iter_f_time:.6f}", simp_dist2, simp_points2, ''])
                fr_t_file.flush()
            cur_delta *= args.reduction

        log(f"7. Writing to minimize_frechet id={idv}...")
        with frechet_lock:
            fr_writer.writerow([idv, best_delta, eps, baseline_dist, best_dist, orig_points, baseline_points, best_points, ''])
            fr_file.flush()


def main():
    parser = argparse.ArgumentParser(description="Benchmark simplify against DP over a range of IDs.")
    parser.add_argument('--a', type=int, help='start id (inclusive)')
    parser.add_argument('--b', type=int, help='end id (inclusive)')
    parser.add_argument('--all', action='store_true', help='run across all ids under data/taxi_simplified/* with original.txt (ignores --a/--b)')
    parser.add_argument('--eps', type=float, nargs='*', default=[0.5], help='epsilon values to test')
    parser.add_argument('--reduction', type=float, default=0.8, help='delta reduction multiplicative factor (default 0.8)')
    parser.add_argument('--max-iters', type=int, default=10, help='max iterations of delta reduction per epsilon')
    parser.add_argument('--derr', type=float, default=0.01, help='delta error to stop')
    parser.add_argument('--release-dir', type=str, default='release', help='path to build/release directory containing binaries')
    parser.add_argument('--frechet-timeout', type=float, default=60.0, help='timeout (s) for frechet computations only')
    parser.add_argument('--algo', type=str, choices=['dp', 'dots'], default='dp', help='algorithm to benchmark against (dp or dots)')
    parser.add_argument('--workers', type=int, default=4, help='Number of parallel workers (default 4)')

    args = parser.parse_args()
    repo_root = Path(__file__).resolve().parent
    release_dir = (repo_root / args.release_dir).resolve()

    main_bin = release_dir / 'main'
    simplify_bin = release_dir / 'simplify'
    dots_bin = repo_root / 'traj-compression' / 'online' / 'DOTS' / 'run_dots'

    if args.algo == 'dp' and not main_bin.exists():
        print(f"Missing binary: {main_bin}", file=sys.stderr)
        return 2
    if args.algo == 'dots' and not dots_bin.exists():
        print(f"Missing binary: {dots_bin}", file=sys.stderr)
        return 2
    if not simplify_bin.exists():
        print(f"Missing binary: {simplify_bin}", file=sys.stderr)
        return 2

    # Build list of ids to process
    if args.all:
        simp_root = repo_root / 'data' / 'taxi_simplified'
        ids = []
        if simp_root.exists() and simp_root.is_dir():
            for p in simp_root.iterdir():
                if p.is_dir() and p.name.isdigit():
                    if (p / 'original.txt').exists():
                        ids.append(int(p.name))
            ids.sort()
        else:
            print(f"Missing directory: {simp_root}", file=sys.stderr)
            return 2
    else:
        if args.a is None or args.b is None:
            print("Provide --a and --b, or use --all", file=sys.stderr)
            return 2
        if args.a > args.b:
            print("Invalid range: --a must be <= --b", file=sys.stderr)
            return 2
        ids = list(range(args.a, args.b + 1))

    # Determine output filenames based on algo
    baseline_csv_name = f"{args.algo}_points.csv"
    frechet_csv_name = f"{args.algo}_frechet.csv"
    baseline_times_csv_name = f"{args.algo}_points_times.csv"
    frechet_times_csv_name = f"{args.algo}_frechet_times.csv"

    baseline_csv = Path(baseline_csv_name)
    frechet_csv = Path(frechet_csv_name)
    # New timing CSVs (do not change existing CSV schemas)
    baseline_times_csv = Path(baseline_times_csv_name)
    frechet_times_csv = Path(frechet_times_csv_name)
    baseline_cols = ['id', 'delta', 'epsilon', 'baseline_dist', 'simp_dist', 'orig_points', 'baseline_points', 'simp_points']
    frechet_cols = ['id', 'best_delta', 'epsilon', 'baseline_dist', 'best_simp_dist', 'orig_points', 'baseline_points', 'best_points', 'note']
    baseline_header = needs_header(baseline_csv, baseline_cols)
    frechet_header = needs_header(frechet_csv, frechet_cols)
    # Timing CSV headers
    baseline_times_cols = ['id', 'epsilon', 'baseline_frechet_time_s', 'simplify_time_s', 'simplified_frechet_time_s']
    frechet_times_cols = ['id', 'epsilon', 'iter', 'delta', 'simplify_time_s', 'frechet_time_s', 'dist', 'points', 'note']
    baseline_times_header = needs_header(baseline_times_csv, baseline_times_cols)
    frechet_times_header = needs_header(frechet_times_csv, frechet_times_cols)

    with baseline_csv.open('a', newline='') as base_file, \
         frechet_csv.open('a', newline='') as fr_file, \
         baseline_times_csv.open('a', newline='') as base_t_file, \
         frechet_times_csv.open('a', newline='') as fr_t_file:
        base_writer = csv.writer(base_file)
        fr_writer = csv.writer(fr_file)
        base_t_writer = csv.writer(base_t_file)
        fr_t_writer = csv.writer(fr_t_file)
        if baseline_header:
            base_writer.writerow(baseline_cols)
        if frechet_header:
            fr_writer.writerow(frechet_cols)
        if baseline_times_header:
            base_t_writer.writerow(baseline_times_cols)
        if frechet_times_header:
            fr_t_writer.writerow(frechet_times_cols)

        # Context packing for workers
        binaries = (main_bin, simplify_bin, dots_bin)
        writers = (base_writer, fr_writer, base_t_writer, fr_t_writer)
        files = (base_file, fr_file, base_t_file, fr_t_file)
        
        points_lock = threading.Lock()
        frechet_lock = threading.Lock()
        base_times_lock = threading.Lock()
        frechet_times_lock = threading.Lock()
        
        print_lock = threading.Lock()
        locks = (points_lock, frechet_lock, base_times_lock, frechet_times_lock, print_lock)
        csv_ctx = (writers, files, locks)

        # Use ThreadPoolExecutor for concurrency
        print(f"Starting benchmark with {len(ids)} IDs using {args.workers} workers...")
        
        # Initialize persistent pool for Frechet calculations to avoid library reload overhead
        # and support timeouts via func_timeout in worker processes.
        with multiprocessing.Pool(processes=args.workers) as frechet_pool:
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
                futures = [
                    executor.submit(process_id, idv, args, repo_root, binaries, csv_ctx, frechet_pool)
                    for idv in ids
                ]
                for future in concurrent.futures.as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"Worker exception: {e}", file=sys.stderr)

    print(f"Done. Baseline results: {baseline_csv} | Min Frechet results: {frechet_csv}")
    return 0


if __name__ == '__main__':
    # Optimization: Use 'forkserver' to allow preloading of libraries (numpy, frechetlib).
    # This avoids the high cost of re-importing them in every "spawned" timeout process,
    # while solving the safety issues of "fork" (which is unsafe with threads).
    if sys.platform != 'win32':
        try:
            import multiprocessing
            multiprocessing.set_start_method('forkserver', force=True)
            # Preload the heavy libraries in the forkserver process
            multiprocessing.set_forkserver_preload(['numpy', 'frechetlib.continuous_frechet'])
        except Exception:
            pass # Fallback to default if forkserver unavailable

    sys.exit(main())
